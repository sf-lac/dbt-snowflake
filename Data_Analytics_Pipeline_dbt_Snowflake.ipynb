{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Data analytics pipeline for Financial Services data leveraging dbt and Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00ef53-e1b3-4bdc-971f-ff3acc262d32",
   "metadata": {
    "collapsed": false,
    "name": "dbt_pipeline_overview"
   },
   "source": [
    "Build scalable data pipeline leveraging dbt and Snowflake to analyze trading performance of a company having trading desks to perform transactions for buying and selling financial intruments in different regions.\n",
    "\n",
    "The dbt pipeline transforms Finance & Economics data product available in Snowflake Marketplace, establishes data tests, generates documentation, and orchestrates deployment to production.\n",
    "\n",
    "The dbt transformations include:\n",
    "\n",
    "- Stock trading history\n",
    "- Foreign currency exchange rates\n",
    "- Trading books with active trades of financial instruments\n",
    "- Market Volume & Profit and Loss calculations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6117-cb0b-4f30-810c-d84ab4d7fb66",
   "metadata": {
    "collapsed": false,
    "name": "Setup_dbt"
   },
   "source": [
    "1. Create a new project in dbt Cloud. Navigate to __`Account settings`__ (by clicking on account name in the left side menu), and click __`+ New Project`__.\n",
    "\n",
    "2. Enter a project name and click Continue.\n",
    "\n",
    "3. For the __`warehouse`__, click __`Snowflake`__ then Next to set up dbt-Snowflake connection.\n",
    "\n",
    "4. Enter __`Settings`__ for Snowflake: __`account identifier`__, __`role`__, __`database`__ (e.g., analytics), __`warehouse`__(e.g., transforming) and __`development credentials`__.\n",
    "\n",
    "5. Link the project to a __`GitHub repository`__.\n",
    "\n",
    "6. Select __`Develop`__ -> __`Cloud IDE`__ - this spins up the project for the first time as it establishes the git connection, clones the repo, and tests the connection to the warehouse (Snowflake).\n",
    "\n",
    "7. Click __`Initialize dbt project`__ - this builds out the folder structure with example dbt models. Execute `dbt run` or `dbt test`.\n",
    "\n",
    "8. Click __`Commit and sync`__ to commit the folder structure to GitHub repo `main` branch. \n",
    "\n",
    "9. Click __`+ Create New Branch`__ to check out a new git branch `dev` for development (separated from the main production branch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2ec12-48ea-4430-9902-16f691fccf55",
   "metadata": {
    "collapsed": false,
    "name": "Connect_to_data_source"
   },
   "source": [
    "From `Snowflake Marketplace` search for and get `Finance & Economics` data product that includes macroeconomic indicators, trading volumes and stock prices of US equities and ETFs executed on the Nasdaq, and banking sector data to give users a view of the current state of the economy & financial industry. A single, unified schema joins together datasets from various sources.\n",
    "\n",
    "Sample queries:\n",
    "\n",
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE FINANCE_ECONOMICS;\n",
    "USE SCHEMA CYBERSYN;\n",
    "\n",
    "-- SELECT * FROM STOCK_PRICE_TIMESERIES; -- 78.0M rows\n",
    "\n",
    "SELECT * FROM STOCK_PRICE_TIMESERIES LIMIT 5;\n",
    "\n",
    "SELECT * FROM STOCK_PRICE_TIMESERIES WHERE TICKER = 'AAPL' AND DATE = '2025-05-13';\n",
    "\n",
    "SELECT DISTINCT VARIABLE FROM STOCK_PRICE_TIMESERIES;\n",
    "\n",
    "-- YTD performance of a select group of stocks\n",
    "WITH ytd_performance AS (\n",
    "  SELECT\n",
    "    ticker,\n",
    "    MIN(date) OVER (PARTITION BY ticker) AS start_of_year_date,\n",
    "    FIRST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS start_of_year_price,\n",
    "    MAX(date) OVER (PARTITION BY ticker) AS latest_date,\n",
    "    LAST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS latest_price\n",
    "  FROM cybersyn.stock_price_timeseries\n",
    "  WHERE\n",
    "    ticker IN ('AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA')\n",
    "    AND date >= DATE_TRUNC('YEAR', CURRENT_DATE()) -- Truncates the current date to the start of the year\n",
    "    AND variable_name = 'Post-Market Close'\n",
    ")\n",
    "SELECT\n",
    "  ticker,\n",
    "  start_of_year_date,\n",
    "  start_of_year_price,\n",
    "  latest_date,\n",
    "  latest_price,\n",
    "  (latest_price - start_of_year_price) / start_of_year_price * 100 AS percentage_change_ytd\n",
    "FROM\n",
    "  ytd_performance\n",
    "GROUP BY\n",
    "  ticker, start_of_year_date, start_of_year_price, latest_date, latest_price\n",
    "ORDER BY percentage_change_ytd DESC;\n",
    ";\n",
    "\n",
    "select * from fx_rates_timeseries limit 5;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34b4d9-4e69-467f-a914-b9b3fcd603c9",
   "metadata": {
    "collapsed": false,
    "name": "dbt_project_configuration"
   },
   "source": [
    "Edit generated `dbt_project.yml` configuration file. Name the project. Remove materialization setting for models in the example folder. \n",
    "\n",
    "Delete the example folder from the models folder.\n",
    "\n",
    "Install [`dbt_utils`](https://hub.getdbt.com/dbt-labs/dbt_utils/latest/) package hosted on dbt Packages Hub to use some dbt_utils macros in writing more complex SQL in dbt models.\n",
    "\n",
    "`dbt models` are .sql files that live in the models folder. `dbt macros` are snippets of reusable code written in Jinja, a templating language integrated into dbt. `dbt package` is a dbt project that can be installed onto own dbt project to gain access to the code and use it as own. \n",
    "\n",
    "To install dbt_utils package, create a `packages.yml` file at the same level as the dbt_project.yml file. \n",
    "\n",
    "```yml\n",
    "packages:\n",
    "  - package: dbt-labs/dbt_utils\n",
    "    version: 1.3.0\n",
    "```\n",
    "\n",
    "Run `dbt deps` to install the package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47e411-dc58-427c-aa3e-290865b93350",
   "metadata": {
    "collapsed": false,
    "name": "dbt_sources"
   },
   "source": [
    "Declare`dbt sources`. \n",
    "\n",
    "`Sources` (src) refer to the raw table data that have been built in the Snowflake data platform through a loading process or available directly in Snowflake through Snowflake Marketplace without any ETL.\n",
    "\n",
    "`Staging` folder: The folder where all source configurations and staging models and source configurations are stored. Further subfolders can be used to separate data by data source. \n",
    "\n",
    "In the `models` folder create a `staging` folder and a subfolder finance to hold the dbt sources.\n",
    "\n",
    "Create a `_finance_sources.yml` file in the staging/finance folder.\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: finance_economics\n",
    "    description: Finance and economics data including stock prices\n",
    "    database: finance_economics\n",
    "    schema: cybersyn\n",
    "    tables:\n",
    "      - name: stock_price_timeseries\n",
    "        description: Stock price timeseries data\n",
    "```\n",
    "\n",
    "Commit changes to `dev` branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb5338-3289-43e5-8104-94548985b1ea",
   "metadata": {
    "collapsed": false,
    "name": "Setup_staging_models"
   },
   "source": [
    "Create `staging models`.\n",
    "\n",
    "`Staging` (stg) refers to models that are built directly on top of sources. They have a one-to-one relationship with sources tables and are used for very light transformations (e.g. renaming columns). These models are used to clean and standardize the data before transforming data downstream. They are typically materialized as views.\n",
    "\n",
    "Create a `stg_finance__stock_history.sql` in the models/staging/finance folder\n",
    "\n",
    "```sql\n",
    "with source as (\n",
    "\n",
    "    select * from {{source('finance_economics','stock_price_timeseries')}}\n",
    "), \n",
    "\n",
    "renamed as (\n",
    "\n",
    "    select \n",
    "\n",
    "        TICKER as company_symbol,\n",
    "        ASSET_CLASS as asset_class,\n",
    "        PRIMARY_EXCHANGE_CODE as stock_exchange_code,\n",
    "        PRIMARY_EXCHANGE_NAME as stock_exchange_name,\n",
    "        VARIABLE as indicator,\n",
    "        VARIABLE_NAME as indicator_name, \n",
    "        DATE as stock_date,\n",
    "        VALUE as stock_value\n",
    "\n",
    "    from source \n",
    "\n",
    ") \n",
    "\n",
    "select * from renamed\n",
    "```\n",
    "\n",
    "Click `Preview` and `Compile` buttons. View the `Lineage`.\n",
    "\n",
    "Commit changes to `dev` branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6db1e-3e71-41d6-a5cd-54d2aa10d179",
   "metadata": {
    "collapsed": false,
    "name": "Create_view_in_Snowflake"
   },
   "source": [
    "To create the view in Snowflake, run the model in the staging folder with: \n",
    "\n",
    "```bash \n",
    "dbt run --select staging.*\n",
    "```\n",
    "\n",
    "In dbt, the default materialization for a model is a view. This means, when `dbt run` or `dbt build` are executed, all of the models will be built as a view in the Snowflake data platform.\n",
    "\n",
    "Verify dbt model materialized as view in Snowflake:\n",
    "\n",
    "```sql\n",
    "-- verify dbt model materialized as view\n",
    "select * from analytics.information_schema.views\n",
    "where table_name = 'STG_FINANCE__STOCK_HISTORY';\n",
    "\n",
    "select * \n",
    "  from analytics.dbt_lcarabet.stg_finance__stock_history\n",
    " where company_symbol ='AAPL' \n",
    "   and stock_date ='2025-05-13';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186586a3-6b75-4b4b-844a-087cc78a77c4",
   "metadata": {
    "collapsed": false,
    "name": "Setup_intermediate_models"
   },
   "source": [
    "In the dataset, different indicators like close, open, high, low price and trading volume are represented by different rows. Transposing data into columns provides a cleaner data representation for downstream analytics.\n",
    "\n",
    "| INDICATOR |\n",
    "| --------- |\n",
    "all-day_high\n",
    "nasdaq_volume\n",
    "post-market_close\n",
    "pre-market_open\n",
    "all-day_low\n",
    "\n",
    "Create `intermediate models`.\n",
    "\n",
    "In dbt, `intermediate` (int) models refer to any models that exist between final fact and dimension tables. They are built on staging models rather than directly on sources to leverage the data cleaning done in the staging layer.\n",
    "\n",
    "`Marts` folder: The folder where all intermediate, fact, and dimension models are stored. Further subfolders can be used to separate data by business function.\n",
    "\n",
    "Create a `marts` folder and `core/intermediate` subfolder in `models`.\n",
    "\n",
    "Create a `int_finance__stock_history.sql` file in the models/marts/core/intermediate folder.\n",
    "\n",
    "The model uses the `dbt_utils.pivot() SQL generator macro` to transpose the dataset from rows to columns, and `dbt_utils.get_column_values() introspective macro` to list the column values dynamically.\n",
    "\n",
    "```sql\n",
    "with stock_history as (\n",
    "\n",
    "    select * from {{ ref('stg_finance__stock_history') }}\n",
    "        where indicator in ('post-market_close', 'pre-market_open','all-day_high','all-day_low', 'nasdaq_volume') \n",
    "\n",
    "),\n",
    "\n",
    "pivoted as (\n",
    "\n",
    "    select \n",
    "        company_symbol, \n",
    "        asset_class, \n",
    "        stock_exchange_name, \n",
    "        stock_date,         \n",
    "        {{ dbt_utils.pivot(\n",
    "      column = 'indicator',\n",
    "      values = dbt_utils.get_column_values(ref('stg_finance__stock_history'), 'indicator'),\n",
    "      then_value = 'stock_value'\n",
    "            ) }}\n",
    "    \n",
    "    from stock_history\n",
    "    group by company_symbol, asset_class, stock_exchange_name, stock_date\n",
    ")\n",
    "\n",
    "select * from pivoted\n",
    "\n",
    "```\n",
    "Click Compile to view compiled code, results and model lineage.\n",
    "\n",
    "The code makes use of the `ref` Jinja function which tells dbt how a model relates to another model. It is useful as:\n",
    "\n",
    "- allows automatic generation of a lineage/directed acyclic graph (DAG) and promotion of code through different environments without having to update the code to change a (hardcoded) database object.\n",
    "\n",
    "- allows to run based on dependencies, a model plus parent models in the DAG.\n",
    "\n",
    "```bash\n",
    "dbt run --select +int_finance__stock_history\n",
    "```\n",
    "Run summary:\n",
    "\n",
    "1. stg_finance__stock_history\n",
    "\n",
    "```bash\n",
    "23:43:01 1 of 2 START sql view model dbt_lcarabet.stg_finance__stock_history ............ [RUN]\n",
    "23:43:02 1 of 2 OK created sql view model dbt_lcarabet.stg_finance__stock_history ....... [SUCCESS 1 in 0.75s]\n",
    "```\n",
    "\n",
    "2. int_finance__stock_history\n",
    "\n",
    "```bash\n",
    "23:43:02 2 of 2 START sql table model dbt_lcarabet.int_finance__stock_history ........... [RUN]\n",
    "23:43:32 2 of 2 OK created sql table model dbt_lcarabet.int_finance__stock_history ...... [`SUCCESS 1` in 30.90s]\n",
    "```\n",
    "\n",
    "Verify int dbt model materialized as table\n",
    "\n",
    "```sql\n",
    "select * from analytics.information_schema.tables\n",
    "where table_name = 'INT_FINANCE__STOCK_HISTORY';\n",
    "\n",
    "select * \n",
    "  from analytics.dbt_lcarabet.int_finance__stock_history\n",
    " where company_symbol ='AAPL' \n",
    "   and stock_date ='2025-05-13';\n",
    "```\n",
    "\n",
    "Commit changes to `dev` branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381bb93a-b622-4a43-ac58-c41944513da8",
   "metadata": {
    "collapsed": false,
    "name": "Setup_additional_staging_and_intermediate_models"
   },
   "source": [
    "Update `_finance_sources.yml` file\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: finance_economics\n",
    "    description: Finance and economics data including stock prices and foreign exchage rates timeseries\n",
    "    database: finance_economics\n",
    "    schema: cybersyn\n",
    "    tables:\n",
    "      - name: stock_price_timeseries\n",
    "        description: Stock price timeseries data - daily prices (open/close, high/low) & trading volumes of US securities executed on the Nasdaq\n",
    "      - name: fx_rates_timeseries\n",
    "        description: Foreign exchange rates timeseries for currency pairs (base and quote currencies)        \n",
    "```\n",
    "\n",
    "Create `stg_finance__fx_rates.sql` staging model\n",
    "\n",
    "```sql\n",
    "with source as (\n",
    "\n",
    "    select * from {{source('finance_economics','fx_rates_timeseries')}}\n",
    "),\n",
    " \n",
    "renamed as (\n",
    " \n",
    "select \n",
    " \n",
    "    BASE_CURRENCY_ID as base_currency_id,\n",
    "    BASE_CURRENCY_NAME as base_currency_name,\n",
    "    QUOTE_CURRENCY_ID as quote_currency_id,\n",
    "    QUOTE_CURRENCY_NAME as quote_currency_name,\n",
    "    BASE_CURRENCY_ID || '/' || QUOTE_CURRENCY_ID as indicator,\n",
    "    VARIABLE_NAME AS indicator_name,\n",
    "    DATE as exchange_date,\n",
    "    VALUE as exchange_value\n",
    "\n",
    " \n",
    "from source \n",
    " \n",
    ") \n",
    " \n",
    "select * from renamed\n",
    "```\n",
    "\n",
    "Create `int_finance__fx_rates.sql` intermediate model. Filter staged data by exchange date to match the start date of the stock prices history series. Stock prices timeseries data range from '2018-05-01' to '2025-05-15' while foreign exchange rates timeseries data range from '1953-08-10' to '2025-05-16'. \n",
    "\n",
    "```sql\n",
    "select * from {{ ref('stg_finance__fx_rates') }} \n",
    " where exchange_date >= '2018-05-01'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb48e4-a3fc-4800-872f-72c8660d6e50",
   "metadata": {
    "collapsed": false,
    "name": "Join_intermediate_models"
   },
   "source": [
    "Create `int_finance__stock_history_major_currency.sql` intermediate model to join trading history and foreign exchange rates timeseries data.\n",
    "\n",
    "```sql\n",
    "with\n",
    "stock_history as (\n",
    "    select * from {{ ref('int_finance__stock_history')}}\n",
    "),\n",
    " \n",
    "fx_rates as (\n",
    "    select * from {{ ref('int_finance__fx_rates') }}\n",
    "),\n",
    " \n",
    "fx_rates_gdp as (\n",
    "    select * from fx_rates\n",
    "        where indicator = 'USD/GBP'   \n",
    "),\n",
    " \n",
    "fx_rates_eur as (\n",
    "    select * from fx_rates\n",
    "        where indicator = 'USD/EUR' \n",
    "),\n",
    " \n",
    "joined as (\n",
    "    select \n",
    "        stock_history.*,\n",
    "        fx_rates_gdp.exchange_value * stock_history.\"pre-market_open\" as gbp_open,       \n",
    "        fx_rates_gdp.exchange_value * stock_history.\"all-day_high\" as gbp_high,     \n",
    "        fx_rates_gdp.exchange_value * stock_history.\"all-day_low\" as gbp_low,   \n",
    "        fx_rates_gdp.exchange_value * stock_history.\"post-market_close\" as gbp_close,     \n",
    "        fx_rates_eur.exchange_value * stock_history.\"pre-market_open\" as eur_open,       \n",
    "        fx_rates_eur.exchange_value * stock_history.\"all-day_high\" as eur_high,     \n",
    "        fx_rates_eur.exchange_value *stock_history.\"all-day_low\" as eur_low,\n",
    "        fx_rates_eur.exchange_value * stock_history.\"post-market_close\" as eur_close    \n",
    "    from stock_history\n",
    "    left join fx_rates_gdp on stock_history.stock_date = fx_rates_gdp.exchange_date\n",
    "    left join fx_rates_eur on stock_history.stock_date = fx_rates_eur.exchange_date\n",
    ")\n",
    "\n",
    "select * from joined\n",
    "```\n",
    "\n",
    "Execute dbt run\n",
    "\n",
    "```bash\n",
    "dbt run --select +int_finance__stock_history_major_currency\n",
    "```\n",
    "\n",
    "Commit changes to `dev` branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e41eaf-fc3f-4d87-b1db-91f86cc74fac",
   "metadata": {
    "collapsed": false,
    "name": "dbt_seeds"
   },
   "source": [
    "In a dbt project, `seeds` are `CSV files` in the seeds directory, that dbt can load into the data warehouse using the `dbt seed` command.\n",
    "\n",
    "Because these CSV files are located in the dbt linked repository, they are version controlled and code reviewable. \n",
    "\n",
    "Seeds are best suited to static data which changes infrequently.\n",
    "\n",
    "Seeds can be referenced in downstream models the same way as referencing models â€” by using the ref function.\n",
    "\n",
    "Create two CSV files for load into Snowflake by running dbt seed command.\n",
    "\n",
    "`seeds/trading_book_gbp.csv`\n",
    "\n",
    "```csv\n",
    "Book,Date,Trader,Instrument,Action,Cost,Currency,Volume,Cost_Per_Share,Stock_exchange_name\n",
    "B-GB1,2024-03-01,Alex V.,AAPL,BUY,-35800,GBP,200,179.0, NASDAQ CAPITAL MARKET\n",
    "B-GB1,2024-03-01,Alex V.,AAPL,BUY,-656010,GBP,3700,177.3,NASDAQ CAPITAL MARKET\n",
    "B-GB1,2024-01-26,Alex V.,AAPL,SELL,97400,GBP,-500,194.8,NASDAQ CAPITAL MARKET\n",
    "B-GB1,2024-01-22,Alex V.,AAPL,BUY,-187866,GBP,980,191.7,NASDAQ CAPITAL MARKET\n",
    "B-GB1,2024-01-22,Eugene C.,AAPL,SELL,97655150,GBP,-50,195.3,NASDAQ CAPITAL MARKET\n",
    "B-GB1,2023-08-31,Eugene C.,AAPL,BUY,-18760,GBP,100,187.6,NASDAQ CAPITAL MARKET\n",
    "B-GB1,2023-08-31,Eugene C.,AAPL,BUY,-9355,GBP,50,187.1,NASDAQ CAPITAL MARKET\n",
    "```\n",
    "`seeds/trading_book_eur.csv`\n",
    "\n",
    "```csv\n",
    "Book,Date,Trader,Instrument,Action,Cost,Currency,Volume,Cost_Per_Share,Stock_exchange_name\n",
    "B-EU1,2024-03-01,Mark O.,AAPL,BUY,-35800,EUR,200,179.0, NASDAQ CAPITAL MARKET\n",
    "B-EU1,2024-03-01,Mark O.,AAPL,BUY,-656010,EUR,3700,177.3,NASDAQ CAPITAL MARKET\n",
    "B-EU1,2024-01-22,Mark O.,AAPL,BUY,-187866,EUR,980,191.7,NASDAQ CAPITAL MARKET\n",
    "B-EU1,2023-08-31,Mark O.,AAPL,BUY,-18760,EUR,100,187.6,NASDAQ CAPITAL MARKET\n",
    "```\n",
    "Verify loading into Snowflake\n",
    "\n",
    "```sql\n",
    "select * from analytics.dbt_lcarabet.trading_book_gbp;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd1cbd-468b-4c94-9316-961474c72cdc",
   "metadata": {
    "collapsed": false,
    "name": "Model_seeded_data"
   },
   "source": [
    "Model seeded data by unioning the seeds together using `dbt_utils.union_relations() macro` which aligns attributes by name and type, and combines the datasets with UNION ALL. Less code, and dynamic - if new columns were to be added to ref models, dbt takes care of it.\n",
    "\n",
    "Create `int_finance__trading_books_unioned.sql` intermediate model.\n",
    "\n",
    "```sql\n",
    "with \n",
    "unioned as (\n",
    "    {{ dbt_utils.union_relations(\n",
    "        relations=[ref('trading_book_gbp'), ref('trading_book_eur')]\n",
    "    ) }}\n",
    " \n",
    "),\n",
    " \n",
    "renamed as (\n",
    "    select      \n",
    "        Book,\n",
    "        Date as book_date,\n",
    "        Trader,\n",
    "        Instrument,\n",
    "        Action as book_action,\n",
    "        Cost,\n",
    "        Currency,\n",
    "        Volume,\n",
    "        Cost_Per_Share,\n",
    "        Stock_exchange_name\n",
    "    from unioned \n",
    ")\n",
    " \n",
    "select * from renamed\n",
    "\n",
    "```\n",
    "\n",
    "Create the intermediate model by running\n",
    "\n",
    "```bash\n",
    "dbt run --select int_finance__trading_books_unioned\n",
    "```\n",
    "```bash\n",
    "02:54:13 1 of 1 START sql table model dbt_lcarabet.int_finance__trading_books_unioned ... [RUN]\n",
    "02:54:14 1 of 1 OK created sql table model dbt_lcarabet.int_finance__trading_books_unioned  [SUCCESS 1 in 1.87s]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be52ed-e366-44e8-a388-b718291de558",
   "metadata": {
    "collapsed": false,
    "name": "Intermediate_analytics"
   },
   "source": [
    "Enhance `trading activities` beyond `BUY` and `SELL` to include `HOLD`.\n",
    "\n",
    "First, create `int_finance__trading_daily_frequency_instrument_position.sql` intermediate model to calculate distinct total_shares per day per trader. Makes use of `dbt_utils.group_by() macro` that lists all columns to group by dynamically.\n",
    "\n",
    "```sql\n",
    "with \n",
    "stock_history as (\n",
    "    select * from {{ ref('int_finance__stock_history_major_currency') }} \n",
    "), \n",
    "\n",
    "unioned_book as (\n",
    "    select * from {{ ref('int_finance__trading_books_unioned') }}\n",
    "),\n",
    "\n",
    "cst_market_days as (\n",
    "    select distinct stock_date\n",
    "        from stock_history\n",
    "        where stock_history.stock_date >= (select min(book_date) as min_dt from  unioned_book)\n",
    "),\n",
    "\n",
    "joined as (\n",
    "    select \n",
    "        cst_market_days.stock_date,\n",
    "        unioned_book.trader,\n",
    "        unioned_book.stock_exchange_name,\n",
    "        unioned_book.instrument,\n",
    "        unioned_book.book,\n",
    "        unioned_book.currency,\n",
    "        sum(unioned_book.volume) as total_shares\n",
    "    from cst_market_days\n",
    "    inner join unioned_book on unioned_book.book_date = cst_market_days.stock_date\n",
    "    where unioned_book.book_date <= cst_market_days.stock_date\n",
    "    {{ dbt_utils.group_by(6) }}\n",
    ")\n",
    "\n",
    "select * from joined\n",
    "\n",
    "```\n",
    "\n",
    "Full trading activities, create `int_finance__trading_daily_frequency_instrument_position_with_trades.sql` intermediate model.\n",
    "\n",
    "```sql\n",
    "with unioned_book as (    \n",
    "    \n",
    "    select * from {{ ref('int_finance__trading_books_unioned') }}\n",
    "\n",
    "),\n",
    " \n",
    "daily_position as (\n",
    "    select * from {{ ref('int_finance__trading_daily_frequency_instrument_position') }}\n",
    "),\n",
    " \n",
    "unioned as (\n",
    "    select \n",
    "        book,\n",
    "        book_date,\n",
    "        trader,\n",
    "        instrument,\n",
    "        book_action,\n",
    "        cost, \n",
    "        currency,\n",
    "        volume, \n",
    "        cost_per_share, \n",
    "        stock_exchange_name,\n",
    "        sum(unioned_book.volume) \n",
    "            over(\n",
    "                partition by \n",
    "                    instrument, \n",
    "                    stock_exchange_name, \n",
    "                    trader \n",
    "                order by \n",
    "                    unioned_book.book_date rows unbounded preceding) \n",
    "                        as total_shares\n",
    "    from unioned_book  \n",
    " \n",
    "    union all \n",
    " \n",
    "    select  \n",
    "        book,\n",
    "        stock_date as book_date, \n",
    "        trader, \n",
    "        instrument, \n",
    "        'HOLD' as book_action,\n",
    "        0 as cost,\n",
    "        currency, \n",
    "        0 as volume, \n",
    "        0 as cost_per_share,\n",
    "        stock_exchange_name,\n",
    "        total_shares\n",
    "    from daily_position\n",
    "    where (book_date,trader,instrument,book,stock_exchange_name) \n",
    "        not in \n",
    "        (select book_date,trader,instrument,book,stock_exchange_name\n",
    "            from unioned_book\n",
    "        )\n",
    ")\n",
    " \n",
    "select * from unioned\n",
    "```\n",
    "\n",
    "Build int_finance__trading_books_unioned and it's child models. Execute\n",
    "\n",
    "```bash\n",
    "dbt run --select int_finance__trading_books_unioned+\n",
    "```\n",
    "\n",
    "Verify in Snowflake:\n",
    "\n",
    "```sql\n",
    "select * \n",
    "from analytics.dbt_lcarabet.int_finance__trading_daily_frequency_instrument_position_with_trades\n",
    "where trader = 'Alex V.'\n",
    "order by book_date\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf153442-cd64-4cf6-a7bc-8daa3abfa050",
   "metadata": {
    "collapsed": false,
    "name": "Create_facts_model"
   },
   "source": [
    "Create `core/facts/fct_finance__trading_mv_pnl.sql` `facts` model to calculate Market Value and Profit and Loss (PnL) change over time. Tracking PnL over time allows investors and businesses to understand their financial performance, identify trends and make more informed decisions. \n",
    "\n",
    "```sql\n",
    "{{ \n",
    "config(\n",
    "      tags = 'core'\n",
    "      ) \n",
    "}}\n",
    "\n",
    "with\n",
    "daily_positions as (\n",
    "    select * from {{ ref('int_finance__trading_daily_frequency_instrument_position_with_trades' )}}\n",
    "\n",
    "),\n",
    "\n",
    "stock_history as (\n",
    "    select * from {{ ref('int_finance__stock_history_major_currency') }}\n",
    "\n",
    "),\n",
    "\n",
    "joined as (\n",
    "    select \n",
    "        daily_positions.instrument, \n",
    "        daily_positions.stock_exchange_name, \n",
    "        daily_positions.book_date, \n",
    "        daily_positions.trader, \n",
    "        daily_positions.volume,\n",
    "        daily_positions.cost, \n",
    "        daily_positions.total_shares,\n",
    "        daily_positions.cost_per_share,\n",
    "        daily_positions.currency,\n",
    "        sum(cost) over(\n",
    "                partition by \n",
    "                    daily_positions.instrument, \n",
    "                    daily_positions.stock_exchange_name, \n",
    "                    trader \n",
    "                order by\n",
    "                    daily_positions.book_date rows unbounded preceding \n",
    "                    )\n",
    "                as cash_cumulative,\n",
    "       case when daily_positions.currency = 'GBP' then gbp_close\n",
    "            when daily_positions.currency = 'EUR' then eur_close\n",
    "            else 'post-market_close'\n",
    "       end AS close_price_matching_currency, \n",
    "       daily_positions.total_shares  * close_price_matching_currency as market_value, \n",
    "       daily_positions.total_shares  * close_price_matching_currency + cash_cumulative as PnL\n",
    "   from daily_positions\n",
    "   inner join stock_history \n",
    "      on daily_positions.instrument = stock_history.company_symbol \n",
    "     and stock_history.stock_date = daily_positions.book_date \n",
    "     and daily_positions.stock_exchange_name = stock_history.stock_exchange_name\n",
    ")\n",
    "\n",
    "select * from joined\n",
    "```\n",
    "\n",
    "Run model\n",
    "\n",
    "```bash\n",
    "dbt run --select fct_finance__trading_mv_pnl\n",
    "```\n",
    "\n",
    "The model was materialized as a table. As the datasets are getting larger and larger, the runtimes are getting longer. To save on the query times when the table is queried, the size of the warehouse could be increased or, more efficiently, materialize the model as an incremental table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a19f1-a587-432b-b08b-27dcd7dbe6b0",
   "metadata": {
    "collapsed": false,
    "name": "Create_facts_incremental_model"
   },
   "source": [
    "With `incremental materialization`, the model will not be rebuilt each time, but rather only the latest rows transformed and added to the existing table. For incremental materialization, the model configuration is updated and the `is_incremental macro` is included and comes into action for incremental runs (and is ignored during initial run and full_refresh option). Incremental models provide perfomance improvement in production applications.\n",
    "\n",
    "```sql\n",
    "{{ \n",
    "config(\n",
    "      materialized='incremental',\n",
    "      unique_key= 'pk_key',\n",
    "      tags = 'core'\n",
    "      ) \n",
    "}}\n",
    "\n",
    "\n",
    "with\n",
    "daily_positions as (\n",
    "    select * from {{ ref('int_finance__trading_daily_frequency_instrument_position_with_trades' )}}\n",
    "\n",
    "),\n",
    "\n",
    "stock_history as (\n",
    "    select * from {{ ref('int_finance__stock_history_major_currency') }}\n",
    "\n",
    "),\n",
    "\n",
    "joined as (\n",
    "    select \n",
    "        daily_positions.instrument, \n",
    "        daily_positions.stock_exchange_name, \n",
    "        daily_positions.book_date, \n",
    "        daily_positions.trader, \n",
    "        daily_positions.volume,\n",
    "        daily_positions.cost, \n",
    "        daily_positions.total_shares,\n",
    "        daily_positions.cost_per_share,\n",
    "        daily_positions.currency,\n",
    "        sum(cost) over(\n",
    "                partition by \n",
    "                    daily_positions.instrument, \n",
    "                    daily_positions.stock_exchange_name, \n",
    "                    trader \n",
    "                order by\n",
    "                    daily_positions.book_date rows unbounded preceding \n",
    "                    )\n",
    "                as cash_cumulative,\n",
    "       case when daily_positions.currency = 'GBP' then gbp_close\n",
    "            when daily_positions.currency = 'EUR' then eur_close\n",
    "            else 'post-market_close'\n",
    "       end AS close_price_matching_currency, \n",
    "       daily_positions.total_shares  * close_price_matching_currency as market_value, \n",
    "       daily_positions.total_shares  * close_price_matching_currency + cash_cumulative as PnL\n",
    "   from daily_positions\n",
    "   inner join stock_history \n",
    "      on daily_positions.instrument = stock_history.company_symbol \n",
    "     and stock_history.stock_date = daily_positions.book_date \n",
    "     and daily_positions.stock_exchange_name = stock_history.stock_exchange_name\n",
    "),\n",
    "\n",
    "joined_primary_key as (\n",
    " \n",
    "    select \n",
    " \n",
    "        {{ dbt_utils.surrogate_key([\n",
    "                'trader', \n",
    "                'instrument', \n",
    "                'book_date', \n",
    "                'stock_exchange_name',\n",
    "                'PnL', \n",
    "            ]) }} as pk_key,\n",
    "                *\n",
    " \n",
    "    from joined \n",
    ")\n",
    "\n",
    "select * from joined_primary_key\n",
    "\n",
    "{% if is_incremental() %}\n",
    "  -- this filter will only be applied on an incremental run\n",
    "  -- this is the database representation of the current model\n",
    "   where book_date > (select max(book_date) from {{ this }})\n",
    " \n",
    "{% endif %}\n",
    "```\n",
    "\n",
    "Run model twice with\n",
    "\n",
    "```bash\n",
    "dbt run --select fct_finance__trading_mv_pnl_incremental\n",
    "```\n",
    "For the first run, the where clause is not built into the sql statement. The first run of an incremental model builds the table to which new rows will be added to in subsequent runs.\n",
    "\n",
    "```sql\n",
    "joined_primary_key as (\n",
    " \n",
    "    select \n",
    " \n",
    "        md5(cast(coalesce(cast(trader as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(instrument as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(book_date as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(stock_exchange_name as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(PnL as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as pk_key,\n",
    "                *\n",
    " \n",
    "    from joined \n",
    ")\n",
    "\n",
    "select * from joined_primary_key\n",
    "```\n",
    "\n",
    "```bash\n",
    "00:37:51 SQL status: SUCCESS 1 in 0.163 seconds\n",
    "00:37:51 1 of 1 OK created sql incremental model dbt_lcarabet.fct_finance__trading_mv_pnl_incremental  [SUCCESS 1 in 2.23s]\n",
    "00:37:51 Finished running node model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\n",
    "```\n",
    "\n",
    "In subsequent runs, dbt is including the where clause to create a temporary table merged then into the existing table. \n",
    "\n",
    "```sql\n",
    "joined_primary_key as (\n",
    " \n",
    "    select \n",
    " \n",
    "        md5(cast(coalesce(cast(trader as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(instrument as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(book_date as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(stock_exchange_name as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(PnL as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as pk_key,\n",
    "                *\n",
    " \n",
    "    from joined \n",
    ")\n",
    "\n",
    "select * from joined_primary_key\n",
    "\n",
    "\n",
    "  -- this filter will only be applied on an incremental run\n",
    "  -- this is the database representation of the current model\n",
    "   where book_date > (select max(book_date) from analytics.dbt_lcarabet.fct_finance__trading_mv_pnl_incremental)\n",
    "```\n",
    "```bash\n",
    "00:41:22 Using snowflake connection \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"\n",
    "00:41:22 On model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental: /* {\"app\": \"dbt\", \"dbt_version\": \"2025.5.13+c9cbe0f\", \"profile_name\": \"user\", \"target_name\": \"default\", \"node_id\": \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"} */\n",
    "merge into analytics.dbt_lcarabet.fct_finance__trading_mv_pnl_incremental as DBT_INTERNAL_DEST\n",
    "        using analytics.dbt_lcarabet.fct_finance__trading_mv_pnl_incremental__dbt_tmp as DBT_INTERNAL_SOURCE\n",
    "        on ((DBT_INTERNAL_SOURCE.pk_key = DBT_INTERNAL_DEST.pk_key))\n",
    "\n",
    "    \n",
    "    when matched then update set\n",
    "        \"PK_KEY\" = DBT_INTERNAL_SOURCE.\"PK_KEY\",\"INSTRUMENT\" = DBT_INTERNAL_SOURCE.\"INSTRUMENT\",\"STOCK_EXCHANGE_NAME\" = DBT_INTERNAL_SOURCE.\"STOCK_EXCHANGE_NAME\",\"BOOK_DATE\" = DBT_INTERNAL_SOURCE.\"BOOK_DATE\",\"TRADER\" = DBT_INTERNAL_SOURCE.\"TRADER\",\"VOLUME\" = DBT_INTERNAL_SOURCE.\"VOLUME\",\"COST\" = DBT_INTERNAL_SOURCE.\"COST\",\"TOTAL_SHARES\" = DBT_INTERNAL_SOURCE.\"TOTAL_SHARES\",\"COST_PER_SHARE\" = DBT_INTERNAL_SOURCE.\"COST_PER_SHARE\",\"CURRENCY\" = DBT_INTERNAL_SOURCE.\"CURRENCY\",\"CASH_CUMULATIVE\" = DBT_INTERNAL_SOURCE.\"CASH_CUMULATIVE\",\"CLOSE_PRICE_MATCHING_CURRENCY\" = DBT_INTERNAL_SOURCE.\"CLOSE_PRICE_MATCHING_CURRENCY\",\"MARKET_VALUE\" = DBT_INTERNAL_SOURCE.\"MARKET_VALUE\",\"PNL\" = DBT_INTERNAL_SOURCE.\"PNL\"\n",
    "    \n",
    "\n",
    "    when not matched then insert\n",
    "        (\"PK_KEY\", \"INSTRUMENT\", \"STOCK_EXCHANGE_NAME\", \"BOOK_DATE\", \"TRADER\", \"VOLUME\", \"COST\", \"TOTAL_SHARES\", \"COST_PER_SHARE\", \"CURRENCY\", \"CASH_CUMULATIVE\", \"CLOSE_PRICE_MATCHING_CURRENCY\", \"MARKET_VALUE\", \"PNL\")\n",
    "    values\n",
    "        (\"PK_KEY\", \"INSTRUMENT\", \"STOCK_EXCHANGE_NAME\", \"BOOK_DATE\", \"TRADER\", \"VOLUME\", \"COST\", \"TOTAL_SHARES\", \"COST_PER_SHARE\", \"CURRENCY\", \"CASH_CUMULATIVE\", \"CLOSE_PRICE_MATCHING_CURRENCY\", \"MARKET_VALUE\", \"PNL\")\n",
    "\n",
    ";\n",
    "00:41:22 SQL status: SUCCESS 0 in 0.731 seconds\n",
    "00:41:22 Using snowflake connection \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"\n",
    "00:41:22 On model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental: /* {\"app\": \"dbt\", \"dbt_version\": \"2025.5.13+c9cbe0f\", \"profile_name\": \"user\", \"target_name\": \"default\", \"node_id\": \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"} */\n",
    "COMMIT\n",
    "00:41:23 SQL status: SUCCESS 1 in 0.345 seconds\n",
    "00:41:23 Applying DROP to: analytics.dbt_lcarabet.fct_finance__trading_mv_pnl_incremental__dbt_tmp\n",
    "00:41:23 Using snowflake connection \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"\n",
    "00:41:23 On model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental: /* {\"app\": \"dbt\", \"dbt_version\": \"2025.5.13+c9cbe0f\", \"profile_name\": \"user\", \"target_name\": \"default\", \"node_id\": \"model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\"} */\n",
    "drop view if exists analytics.dbt_lcarabet.fct_finance__trading_mv_pnl_incremental__dbt_tmp cascade\n",
    "00:41:23 SQL status: SUCCESS 1 in 0.189 seconds\n",
    "00:41:23 1 of 1 OK created sql incremental model dbt_lcarabet.fct_finance__trading_mv_pnl_incremental  [SUCCESS 0 in 2.53s]\n",
    "00:41:23 Finished running node model.dbt_snowflake_finance.fct_finance__trading_mv_pnl_incremental\n",
    "```\n",
    "dbt incremental models provide a way to build tables in Snowflake efficiently, especially for large datasets, by only processing new or changed data instead of rebuilding the entire table each time. \n",
    "\n",
    "Commit to dev branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c989af-4495-42cd-8a60-b57a75067dac",
   "metadata": {
    "collapsed": false,
    "name": "dbt_tests"
   },
   "source": [
    "`dbt testing` allows data/analytics engineers to ensure that SQL transformations produce models that meet certain assertions.\n",
    "\n",
    "In dbt, there are two types of tests: `generic tests` and `singular tests`.\n",
    "\n",
    "- `Generic tests` are used to validate data models and ensure data quality. These tests are predefined and can be applied to any column of the data models to check for common data issues. They are written in YAML files. dbt ships with four `built-in generic tests`: unique, not_null, accepted_values, relationships.\n",
    "    - `Unique tests` to see if every value in a column is unique\n",
    "    - `Not_null tests` to see if every value in a column is not null\n",
    "    - `Accepted_values tests` to make sure every value in a column is equal to a value in a provided list\n",
    "    - `Relationships tests` to ensure that every value in a column exists in a column in another model \n",
    "    \n",
    "- `Singular tests` are data tests defined by writing specific SQL queries that return records which fail the test conditions. These tests are referred to as \"singular\" because they are one-off assertions that are uniquely designed for a single purpose or specific scenario within the data models.\n",
    "\n",
    "Several way to execute the tests:\n",
    "\n",
    "- `dbt test` runs all tests in the dbt project.\n",
    "- `dbt test --select test_type:generic` runs all generic tests.\n",
    "- `dbt test --select test_type:singular` runs all singular tests.\n",
    "- `dbt test --select one_specific_model`\n",
    "\n",
    "Create `intermediate/int_finance.yml` file to configure generic dbt tests for the intermediate models.\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: int_finance__fx_rates\n",
    "    description: \"An intermediate model that filters stg_finance__fx_rates staging model by exchange_date\"\n",
    "    columns:\n",
    "      - name: indicator||exchange_date\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "\n",
    "  - name: int_finance__trading_books_unioned\n",
    "    description: \"An intermediate model that unions seeded data in CSV files\"\n",
    "    columns:\n",
    "      - name: instrument\n",
    "        tests:\n",
    "          - not_null\n",
    "          - relationships:\n",
    "              to: ref('int_finance__stock_history')\n",
    "              field: company_symbol\n",
    "\n",
    "  - name: int_finance__stock_history\n",
    "    description: \"An intermediate model that pivots the stg_finance__stock_history model by indicator\"\n",
    "    columns:\n",
    "      - name: company_symbol||stock_date\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "```\n",
    "\n",
    "Create `staging/finance/stg_finance.yml` to configure generic tests for staging models which makes use of {{doc}} jinja function that references `docs blocks` in the descriptin field of .yml files.\n",
    "\n",
    "```yml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: stg_finance__stock_history\n",
    "    description: Staged stock prices historical data.\n",
    "    columns:      \n",
    "      - name: indicator\n",
    "        description: '{{doc(\"trading_indicator\")}}'\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              values:\n",
    "                - all-day_high\n",
    "                - all-day_low\n",
    "                - pre-market_open\n",
    "                - post-market_close\n",
    "                - nasdaq_volume\n",
    "```\n",
    "Create `staging/finance/trading_indicator.md` to document trading indicator accepted values.\n",
    "\n",
    "```md\n",
    "{% docs trading_indicator %}\n",
    "\n",
    "One of the following values: \n",
    "\n",
    "| status            | definition                                       |\n",
    "|-------------------|--------------------------------------------------|\n",
    "| all-day_high      | Daily high stock price                           |\n",
    "| all-day_low       | Daily low stock price                            |\n",
    "| pre-market_open   | Daily opening price                              |\n",
    "| post-market_close | Daily closing price                              |\n",
    "| nasdaq_volume     | Daily trading volume on stock exchange markets   |\n",
    "\n",
    "{% enddocs %}\n",
    "```\n",
    "\n",
    "Create `tests/assert_stg_finance__fx_rates_exchange_value_positive.sql` for a singular test.\n",
    "\n",
    "```sql\n",
    "-- exchange rates should be >=0\n",
    "select \n",
    "    indicator,\n",
    "    exchange_date,\n",
    "    exchange_value\n",
    "from {{ ref ('stg_finance__fx_rates') }}\n",
    "where exchange_value < 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71041c89-b1d2-4f03-8562-f270976669f2",
   "metadata": {
    "collapsed": false,
    "name": "dbt_run_all_models"
   },
   "source": [
    "`dbt run`\n",
    "\n",
    "```bash\n",
    "06:21:04 Cloud CLI invocation created: fd4c5401-48b2-413c-8ad7-08bf4a9966a6\n",
    "06:21:04 Running dbt...\n",
    "06:21:05 Found 10 models, 2 seeds, 8 data tests, 2 sources, 592 macros\n",
    "06:21:05 \n",
    "06:21:05 Concurrency: 4 threads (target='default')\n",
    "06:21:05 \n",
    "06:21:53 \n",
    "06:21:53 Finished running 1 incremental model, 7 table models, 2 view models in 0 hours 0 minutes and 48.65 seconds (48.65s).\n",
    "06:21:53 \n",
    "06:21:53 Completed successfully\n",
    "06:21:53 \n",
    "06:21:53 Done. PASS=10 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f956a-d6d4-4aec-9601-a98ac1943dc8",
   "metadata": {
    "collapsed": false,
    "name": "Commit_changes_to_dev_branch"
   },
   "source": [
    "Commit all changes to the `dev` branch of GitHub repo. Click `Commit and Sync` one last time in dbt Cloud IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb022a2-5e0e-4de5-b128-369e6aa1a519",
   "metadata": {
    "collapsed": false,
    "name": "Generate_documentation_site"
   },
   "source": [
    "\n",
    "In dbt, models are built in SQL files. These models are documented in YML files (where generic tests also live) inside the same folder as the models. \n",
    "- For models, descriptions can happen at the model, source, or column level.\n",
    "- If a longer form, more styled version of text would provide a strong description, `docs blocks` can be used to render markdown in the generated documentation.\n",
    "\n",
    "\n",
    "`dbt docs generate` command generates a static webpage with a data dictionary by pulling in information from the dbt project as well as the Snowflake information_schema. Facilitates information sharing with internal teams, as it contains model, source, and column descriptions, tags, tests as well as the source and compiled SQL code for each model. It also provides an interactive DAG to visualize the full lineage graph of dbt models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4647fa1-a6cc-4cc6-9f0e-3976698ccb89",
   "metadata": {
    "collapsed": false,
    "name": "Merge_changes_to_main_brach"
   },
   "source": [
    "Merge all changes made in the `development environment` that makes use of `the development schema` (`dbt_lcarabet`) and the `dev branch` to `main branch` so that those changes can be used in `deployment`.\n",
    "\n",
    "In dbt Cloud IDE, click on `Create a pull request on GitHub`. In GitHub, compare changes between dev and main for ability to merge and if everything checks out click `Create pull request` dev-to-main. If no conflicts with main/base branch, merging is performed automatically by GitHub by clicking `Merge pull request` button and `Confirm merge`.\n",
    "\n",
    "Alternatively, merging can be done via command line:\n",
    "\n",
    "- Connect to GitHub repo via https\n",
    "- Clone the repository or update local repository with latest changes\n",
    "```bash\n",
    "git pull origin main\n",
    "```\n",
    "- Switch to the main/base branch of the pull request\n",
    "```bash\n",
    "git checkout main\n",
    "```\n",
    "- Merge the dev/head branch into the main/base branch\n",
    "```bash\n",
    "git merge dev\n",
    "```\n",
    "- Push the changes\n",
    "```bash\n",
    "git push -u origin main\n",
    "```\n",
    "Can then `Delete branch dev` if no longer needed.\n",
    "\n",
    "In dbt, click `Change branch` and `Pull from main`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf4027-637d-490b-bfb1-ffd4cc601fbb",
   "metadata": {
    "collapsed": false,
    "name": "Deployment"
   },
   "source": [
    "`Deployment` in dbt is the process of running dbt on a schedule in a deployment/production environment for use to power dashboards, reporting, and drive key business decision-making processes. The `deployment environment` runs from the `main branch` and uses a dedicated `deployment schema` (e.g., `dbt_production`) different than the development schema. \n",
    "\n",
    "Click on `Deploy` menu in dbt Could IDE to:\n",
    "\n",
    "- Create `deployment environment` -> `Environments` -> Setup general settings for environment name (`Production`) and type (`Deployment`), `Snowflake connection settings` (account/role/database/warehouse), `Snowflake deployment credentials` (authentication and deployment schema: `dbt_production`)\n",
    "\n",
    "- `Schedule a job` to orchestrate the execution of models in production -> `Jobs` -> `Create job` -> \n",
    "    - set `job settings`: name (e.g., 'finance_economics_job'), set environment to 'Production', \n",
    "    - set `execution settings` & `commands` -> dbt seed -> dbt run -> dbt test; and check `Generate docs on run`\n",
    "    - set the `schedule`: intervals, specific hours or cron scheduled job\n",
    "    - set `advanced settings` `threads` (max number of paths) to greater than default 4 threads (e.g., 8) to minimize the runtime at the potential expense on the Snowflake warehouse\n",
    "\n",
    "Turn off scheduling to run the job one time. Save -> Run now -> Click on Run #number to view execution -> Success\n",
    "\n",
    "Verify deployment in Snowflake:\n",
    "\n",
    "```sql\n",
    "-- verify deployment\n",
    "\n",
    "-- two views\n",
    "select * from analytics.information_schema.views\n",
    "where table_schema = 'DBT_PRODUCTION' and table_name like 'STG_FINANCE__%';\n",
    "\n",
    "-- two tables\n",
    "select * from analytics.information_schema.tables\n",
    "where table_schema = 'DBT_PRODUCTION' and table_name like 'TRADING_%';\n",
    "\n",
    "-- six tables\n",
    "select * from analytics.information_schema.tables\n",
    "where table_schema = 'DBT_PRODUCTION' and table_name like 'INT_FINANCE__%';\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "lac@mechatronix.ca",
   "authorId": "7638589351455",
   "authorName": "SWAN20BLACK25",
   "lastEditTime": 1747691459186,
   "notebookId": "clx32pnqp67c67jahfj5",
   "sessionId": "b802c7c1-c780-4d03-baaa-ac0041883639"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
